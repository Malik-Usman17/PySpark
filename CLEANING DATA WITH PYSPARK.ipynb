{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\SPARK\\\\spark-2.4.5-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter1: DATAFRAME DETAILS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: DEFINING A SCHEMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Import * from the **pyspark.sql.types** library.\n",
    "\n",
    "2: Define a new schema using the **StructType** method.\n",
    "\n",
    "3: Define a **StructField** for **name**, **age**, and **city**. Each field should correspond to the correct datatype and not be **nullable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),  #name is a column and False means not nullable\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: USING LAZY PROCESSING\n",
    "\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (**aa_dfw_df**) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "1: Load the Data Frame.\n",
    "\n",
    "2: Add the transformation for **F.lower()** to the **Destination Airport** column.\n",
    "\n",
    "3: Drop the **Destination Airport** column from the Data Frame **aa_dfw_df**. Note the time for these operations to complete.\n",
    "\n",
    "4: Show the Data Frame, noting the time difference for this action to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000012C1E1E2588>\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|         0005|                HNL|                          537|\n",
      "|       01/01/2017|         0007|                OGG|                          498|\n",
      "|       01/01/2017|         0037|                SFO|                          241|\n",
      "|       01/01/2017|         0043|                DTW|                          134|\n",
      "|       01/01/2017|         0051|                STL|                           88|\n",
      "|       01/01/2017|         0060|                MIA|                          149|\n",
      "|       01/01/2017|         0071|                LAX|                          203|\n",
      "|       01/01/2017|         0074|                MEM|                           76|\n",
      "|       01/01/2017|         0081|                DEN|                          123|\n",
      "|       01/01/2017|         0089|                SLC|                          161|\n",
      "|       01/01/2017|         0096|                STL|                           84|\n",
      "|       01/01/2017|         0103|                SJC|                          216|\n",
      "|       01/01/2017|         0119|                OGG|                          514|\n",
      "|       01/01/2017|         0123|                HNL|                          529|\n",
      "|       01/01/2017|         0126|                LGA|                          171|\n",
      "|       01/01/2017|         0132|                EWR|                          188|\n",
      "|       01/01/2017|         0140|                SJC|                          231|\n",
      "|       01/01/2017|         0174|                RDU|                          145|\n",
      "|       01/01/2017|         0176|                BOS|                          184|\n",
      "|       01/01/2017|         0190|                SAT|                           76|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/AA_DFW_2017_Departures_Short.csv.gz')\n",
    "aa_dfw_df.show()\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Date (MM/DD/YYYY): string, Flight Number: string, Destination Airport: string, Actual elapsed time (Minutes): string]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Attribute name \"Date (MM/DD/YYYY)\" contains invalid character(s) among \" ,;{}()\\\\n\\\\t=\". Please use alias to rename it.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\SPARK\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SPARK\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o487.parquet.\n: org.apache.spark.sql.AnalysisException: Attribute name \"Date (MM/DD/YYYY)\" contains invalid character(s) among \" ,;{}()\\n\\t=\". Please use alias to rename it.;\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:583)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$setSchema$2.apply(ParquetWriteSupport.scala:444)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$setSchema$2.apply(ParquetWriteSupport.scala:444)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:444)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.prepareWrite(ParquetFileFormat.scala:111)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a8691cf3492d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Save the df3 DataFrame in Parquet format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AA_DFW_ALL.parquet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\SPARK\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SPARK\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SPARK\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Attribute name \"Date (MM/DD/YYYY)\" contains invalid character(s) among \" ,;{}()\\\\n\\\\t=\". Please use alias to rename it.;'"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/AA_DFW_2017_Departures_Short.csv.gz')\n",
    "#df1.show()\n",
    "df2 = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/AA_DFW_2016_Departures_Short.csv.gz')\n",
    "#df2.show()\n",
    "print(df1)\n",
    "df3 = df1.union(df2)\n",
    "#df3 = df3.withColumn(\"Date(MM-DD-YYYY)\", df3.Date (MM/DD/YYYY))\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: SAVING A DATAFRAME IN PARQUET FORMAT\n",
    "\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The **Parquet** format is a columnar data store, allowing Spark to use **predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset**. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: View the row count of **df** and **df2**.\n",
    "\n",
    "2: Combine **df1** and **df2** in a new DataFrame named **df3** with the union method.\n",
    "\n",
    "3: Save **df3** to a **parquet** file named **AA_DFW_ALL.parquet**.\n",
    "\n",
    "4: Read the **AA_DFW_ALL.parquet** file and show the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 140604\n",
      "DataFrame[Date-MM/DD/YYYY: string, FlightNumber: string, Destination-Airport: string, ActualelapsedtimeMinutes: string]\n",
      "279962\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/AA_DFW_2017_new_Departures_Short.csv')\n",
    "\n",
    "df2 = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/AA_DFW_2016_new_Departures_Short.csv')\n",
    "#print(df1)\n",
    "\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "df3 = df3.withColumnRenamed(\"Flight Number\", \"FlightNumber\")\n",
    "\n",
    "df3 = df3.withColumnRenamed(\"Destination Airport\", \"Destination-Airport\")\n",
    "\n",
    "df3 = df3.withColumnRenamed(\"Actual elapsed time (Minutes)\", \"ActualelapsedtimeMinutes\")\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('E:/STUDY/DATASETS/AA_DFW_ALL.parquet', mode='overwrite')\n",
    "print(df3)\n",
    "\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('E:/STUDY/DATASETS/AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: SQL AND PARQUET\n",
    "\n",
    "1: Import the **AA_DFW_ALL.parquet** file into **flights_df**.\n",
    "\n",
    "2: Use the **createOrReplaceTempView** method to alias the **flights** table.\n",
    "\n",
    "3: Run a Spark SQL query against the **flights** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('E:/STUDY/DATASETS/AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(ActualelapsedtimeMinutes) from flights').collect()[0] #.collect()[0] need tounderstand this\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter2: MANIPULATING DATAFRAMES IN THE REAL WORLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: FILTERING COLUMN CONTENT WITH PYTHON\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame **voter_df** contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the **VOTER_NAME** column.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Show the distinct **VOTER_NAME** entries.\n",
    "\n",
    "2: Filter **voter_df** where the **VOTER_NAME** is 1-20 characters in length.\n",
    "\n",
    "3: Filter out **voter_df** where the **VOTER_NAME** contains an **_**.\n",
    "\n",
    "4: Show the distinct **VOTER_NAME** entries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District              |\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                                           |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|null                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District |\n",
      "|011018__42                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Mark  Clayton                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Casey Thomas                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Sandy  Greyson                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Mark Clayton                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Jennifer S.  Gates                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Tiffinni A. Young                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing   classifications  for the apportionment of costs and the  methods  of  assessing  special  assessments for the services and improvements  to  property  in  the  District;  closing the hearing and  levying  a  special  assessment  on  property  in  the  District       |\n",
      "|B. Adam  McGough                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|Omar Narvaez                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Philip T. Kingston                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Rickey D. Callahan                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Dwaine R. Caraway                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Philip T.  Kingston                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Jennifer S. Gates                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Lee M. Kleinman                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District|\n",
      "|Rickey D.  Callahan                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Carolyn King Arnold                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District               |\n",
      "|Erik Wilson                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District; closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District                 |\n",
      "|Lee Kleinman                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|Mark  Clayton      |\n",
      "|Casey Thomas       |\n",
      "|Sandy  Greyson     |\n",
      "|Mark Clayton       |\n",
      "|Jennifer S.  Gates |\n",
      "|Tiffinni A. Young  |\n",
      "|B. Adam  McGough   |\n",
      "|Omar Narvaez       |\n",
      "|Philip T. Kingston |\n",
      "|Rickey D. Callahan |\n",
      "|Dwaine R. Caraway  |\n",
      "|Philip T.  Kingston|\n",
      "|Jennifer S. Gates  |\n",
      "|Lee M. Kleinman    |\n",
      "|Monica R. Alonzo   |\n",
      "|Rickey D.  Callahan|\n",
      "|Carolyn King Arnold|\n",
      "|Erik Wilson        |\n",
      "|Lee Kleinman       |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/DallasCouncilVoters.csv')\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         VOTER_NAME|\n",
      "+-------------------+\n",
      "|  Jennifer S. Gates|\n",
      "| Philip T. Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|Carolyn King Arnold|\n",
      "|       Scott Griggs|\n",
      "|   B. Adam  McGough|\n",
      "|       Lee Kleinman|\n",
      "|      Sandy Greyson|\n",
      "|  Jennifer S. Gates|\n",
      "| Philip T. Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|Carolyn King Arnold|\n",
      "| Rickey D. Callahan|\n",
      "|  Jennifer S. Gates|\n",
      "|     Sandy  Greyson|\n",
      "| Jennifer S.  Gates|\n",
      "|Philip T.  Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|  Dwaine R. Caraway|\n",
      "|Rickey D.  Callahan|\n",
      "|       Omar Narvaez|\n",
      "|       Kevin Felder|\n",
      "|     Tennell Atkins|\n",
      "|      Mark  Clayton|\n",
      "|       Scott Griggs|\n",
      "|   B. Adam  McGough|\n",
      "|      Scott  Griggs|\n",
      "|   B. Adam  McGough|\n",
      "|    Lee M. Kleinman|\n",
      "|     Sandy  Greyson|\n",
      "| Jennifer S.  Gates|\n",
      "|Philip T.  Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|  Dwaine R. Caraway|\n",
      "|Rickey D.  Callahan|\n",
      "|       Omar Narvaez|\n",
      "|       Kevin Felder|\n",
      "|     Tennell Atkins|\n",
      "|      Mark  Clayton|\n",
      "|      Scott  Griggs|\n",
      "|   B. Adam  McGough|\n",
      "|    Lee M. Kleinman|\n",
      "|     Sandy  Greyson|\n",
      "| Jennifer S.  Gates|\n",
      "|Philip T.  Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|  Dwaine R. Caraway|\n",
      "|Rickey D.  Callahan|\n",
      "|       Omar Narvaez|\n",
      "|       Kevin Felder|\n",
      "|     Sandy  Greyson|\n",
      "| Jennifer S.  Gates|\n",
      "|Philip T.  Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|Rickey D.  Callahan|\n",
      "|       Omar Narvaez|\n",
      "|       Kevin Felder|\n",
      "|     Tennell Atkins|\n",
      "|      Mark  Clayton|\n",
      "|      Scott  Griggs|\n",
      "|   B. Adam  McGough|\n",
      "|    Lee M. Kleinman|\n",
      "|     Sandy  Greyson|\n",
      "| Jennifer S.  Gates|\n",
      "|Philip T.  Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|       Casey Thomas|\n",
      "|Rickey D.  Callahan|\n",
      "|       Kevin Felder|\n",
      "|     Tennell Atkins|\n",
      "|      Mark  Clayton|\n",
      "|      Scott  Griggs|\n",
      "|   B. Adam  McGough|\n",
      "|    Lee M. Kleinman|\n",
      "|     Sandy  Greyson|\n",
      "| Jennifer S.  Gates|\n",
      "|Philip T.  Kingston|\n",
      "|Michael S. Rawlings|\n",
      "|       Adam Medrano|\n",
      "|      Casey  Thomas|\n",
      "|Rickey D.  Callahan|\n",
      "|       Omar Narvaez|\n",
      "|       Kevin Felder|\n",
      "|     Tennell Atkins|\n",
      "|      Mark  Clayton|\n",
      "|      Scott  Griggs|\n",
      "|   B. Adam  McGough|\n",
      "+-------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.select('VOTER_NAME').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: MODIFYING DATAFRAME COLUMNS\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - first_name and last_name. She asks you to split the VOTER_NAME column into words on any space character. You'll treat the last word as the last_name, and all other words as the first_name. You'll be using some new functions in this exercise including **.split(), .size(), and .getItem()**.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Add a new column called **splits** holding the list of possible names.\n",
    "\n",
    "2: Use the **getItem()** method and create a new column called **first_name**.\n",
    "\n",
    "3: Get the last entry of the **splits** list and create a column called **last_name**.\n",
    "\n",
    "4: Drop the **splits** column and show the new **voter_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|\n",
      "+----------+-------------+-------------------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     [Scott, Griggs]|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough| [B., Adam, McGough]|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     [Lee, Kleinman]|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    [Sandy, Greyson]|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|[Carolyn, King, A...|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|[Rickey, D., Call...|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|    [Sandy, Greyson]|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|[Jennifer, S., Ga...|\n",
      "+----------+-------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "voter_df.show()\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "l = ['malik', 'sharjeel', 'umair']\n",
    "print(len(l))\n",
    "\n",
    "import numpy as np\n",
    "print(np.size(l) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.size(l) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: when() EXAMPLE\n",
    "1: Add a column to **voter_df** named **random_val** with the results of the \n",
    "\n",
    "2: **F.rand()** method for any voter with the title **Councilmember**.\n",
    "\n",
    "3: Show some of the DataFrame rows, noting whether the **.when()** clause worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|          random_val|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates| 0.24187069712384035|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.09506244915212914|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.4122105474958815|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.7423475735081858|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.8590369223603771|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|  0.2962483998172717|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|   0.465153012488559|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|  0.6733754310390888|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|  0.4947959894057773|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.9252729167502914|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.20790369723599078|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|0.001758373501031...|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas| 0.46724535979441817|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.8664008524702935|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|  0.6317850667970585|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.9718188154160234|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson| 0.41469140110033753|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates| 0.11077607765736808|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val', F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: WHEHN / OTHERWISE\n",
    "1: Add a column to **voter_df** named **random_val** with the results of the **F.rand()** method for any voter with the title **Councilmember**. Set **random_val** to 2 for the **Mayor**. Set any other title to the value 0.\n",
    "\n",
    "2: Show some of the Data Frame rows, noting whether the clauses worked.\n",
    "\n",
    "3: Use the **.filter** clause to find 0 in **random_val**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|          random_val|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|0.006641414161357773|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|  0.4967686417936066|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                 2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.5358170203472812|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.9733400220184432|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.2293476812801355|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs| 0.09690615941022906|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough| 0.18239797441544126|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|  0.3425374808267956|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|0.009457792244724983|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates| 0.22103515816731112|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|0.004310959002279291|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                 2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.0879650403069816|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.5592869534060679|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.3073743837531372|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|  0.7983454062796397|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|   0.810587357965535|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|  0.8189944380911794|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|   0.331719909261313|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|    Casey  Thomas|     Casey|   Thomas|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: USER DEFINED FUNCTIONS IN SPARK\n",
    "\n",
    "1: Edit the **getFirstAndMiddle()** function to return a space separated string of names, except the last entry in the names list.\n",
    "\n",
    "2: Define the function as a user-defined function. It should return a string type.\n",
    "\n",
    "3: Create a new column on **voter_df** called **first_and_middle_name** using your UDF.\n",
    "\n",
    "4: Drop the \"first_name\" and \"splits\" columns (on separate lines), then show the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|              splits|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|[Jennifer, S., Ga...|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|[Philip, T., King...|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|[Michael, S., Raw...|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|     [Adam, Medrano]|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|     [Casey, Thomas]|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|[Carolyn, King, A...|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|     [Scott, Griggs]|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough| [B., Adam, McGough]|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|     [Lee, Kleinman]|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|    [Sandy, Greyson]|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|[Jennifer, S., Ga...|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|[Philip, T., King...|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|[Michael, S., Raw...|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|     [Adam, Medrano]|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|     [Casey, Thomas]|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|[Carolyn, King, A...|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|[Rickey, D., Call...|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|[Jennifer, S., Ga...|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|    [Sandy, Greyson]|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|[Jennifer, S., Ga...|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "voter_df.show()\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e07ff46b4ae1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Create a new column using your UDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mvoter_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoter_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'first_and_middle_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mudfFirstAndMiddle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoter_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Drop the unnecessary columns then show the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SPARK\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1304\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1305\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'splits'"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, F.StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Drop the unnecessary columns then show the DataFrame\n",
    "voter_df = voter_df.drop('first_name')\n",
    "voter_df = voter_df.drop('splits')\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDING AN ID FIELD\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the **unique** voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame **partition** - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Select the unique entries from the column **VOTER NAME** and create a new DataFrame called **voter_df**.\n",
    "\n",
    "2: Count the rows in the **voter_df** DataFrame.\n",
    "\n",
    "3: Add a ROW_ID column using the appropriate Spark function.\n",
    "\n",
    "4: Show the rows with the 10 highest ROW_IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').options(Header=True).load('E:/STUDY/DATASETS/DallasCouncilVotes.csv.gz')\n",
    "\n",
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct() #use distinct for unique entries\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDs WITH DIFFERENT PARTITIONS\n",
    "You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "To check the number of partitions, use the method **.rdd.getNumPartitions()** on a DataFrame.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Print the number of partitions on each DataFrame.\n",
    "\n",
    "2: Add a **ROW_ID** field to each DataFrame.\n",
    "\n",
    "3: Show the top 10 IDs in each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 200 partitions in the voter_df DataFrame.\n",
      "\n",
      "\n",
      "There are 200 partitions in the voter_df_single DataFrame.\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "#print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE ID TRICKS \n",
    "1: Determine the highest **ROW_ID** in **voter_df_march** and save it in the variable **previous_max_ID**. The statement **.rdd.max()[0]** will get the maximum ID.\n",
    "\n",
    "2: Add a **ROW_ID** column to **voter_df_april** starting at the value of **previous_max_ID**.\n",
    "\n",
    "3: Show the **ROW_ID**'s from both Data Frames and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|       ROW_ID|\n",
      "+-------------+\n",
      "|   8589934592|\n",
      "|  25769803776|\n",
      "|  34359738368|\n",
      "|  42949672960|\n",
      "|  51539607552|\n",
      "| 103079215104|\n",
      "| 111669149696|\n",
      "| 231928233984|\n",
      "| 240518168576|\n",
      "| 360777252864|\n",
      "| 395136991232|\n",
      "| 601295421440|\n",
      "| 635655159808|\n",
      "| 670014898176|\n",
      "| 807453851648|\n",
      "| 850403524608|\n",
      "| 944892805120|\n",
      "| 962072674304|\n",
      "|1005022347264|\n",
      "|1047972020224|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+\n",
      "|       ROW_ID|\n",
      "+-------------+\n",
      "|1717986918400|\n",
      "|1735166787584|\n",
      "|1743756722176|\n",
      "|1752346656768|\n",
      "|1760936591360|\n",
      "|1812476198912|\n",
      "|1821066133504|\n",
      "|1941325217792|\n",
      "|1949915152384|\n",
      "|2070174236672|\n",
      "|2104533975040|\n",
      "|2310692405248|\n",
      "|2345052143616|\n",
      "|2379411881984|\n",
      "|2516850835456|\n",
      "|2559800508416|\n",
      "|2654289788928|\n",
      "|2671469658112|\n",
      "|2714419331072|\n",
      "|2757369004032|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter3: IMPROVING PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: CACHING A DATAFRAME\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Cache the unique rows in the **departures_df** DataFrame.\n",
    "\n",
    "2: Perform a count query on **departures_df**, noting how long the operation takes.\n",
    "\n",
    "3: Count the rows again, noting the variance in time of a cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 6.884262 seconds\n",
      "Counting 139358 rows again took 2.179404 seconds\n"
     ]
    }
   ],
   "source": [
    "departures_df = spark.read.csv('E:/STUDY/DATASETS/AA_DFW_2017_new_Departures_Short.csv', header=True)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider why the first run takes longer even though you've told it to **cache()** the DataFrame. Remember that even though you've applied the caching transformation, it doesn't take effect until an action is run. The action instantiates the caching after the **distinct()** function completes. The second time, there is no need to recalculate anything so it returns almost immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: REMOVING A DATAFRAME FROM CACHE\n",
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Check the caching status on the **departures_df** DataFrame.\n",
    "\n",
    "2: Remove the **departures_df** DataFrame from the cache.\n",
    "\n",
    "3: Validate the caching status again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: FILE IMPORT PERFORMANCE (### Discuss)\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: **departures_full.txt.gz** and **departures_xxx.txt.gz** where xxx is 000 - 013. The same number of rows is split between each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-------------------+--------------------+\n",
      "|            _c0|          _c1|                _c2|                 _c3|\n",
      "+---------------+-------------+-------------------+--------------------+\n",
      "|Date-MM/DD/YYYY|Flight Number|Destination Airport|Actual elapsed ti...|\n",
      "|       01-01-17|            5|                HNL|                 537|\n",
      "|       01-01-17|            7|                OGG|                 498|\n",
      "|       01-01-17|           37|                SFO|                 241|\n",
      "|       01-01-17|           43|                DTW|                 134|\n",
      "|       01-01-17|           51|                STL|                  88|\n",
      "|       01-01-17|           60|                MIA|                 149|\n",
      "|       01-01-17|           71|                LAX|                 203|\n",
      "|       01-01-17|           74|                MEM|                  76|\n",
      "|       01-01-17|           81|                DEN|                 123|\n",
      "|       01-01-17|           89|                SLC|                 161|\n",
      "|       01-01-17|           96|                STL|                  84|\n",
      "|       01-01-17|          103|                SJC|                 216|\n",
      "|       01-01-17|          119|                OGG|                 514|\n",
      "|       01-01-17|          123|                HNL|                 529|\n",
      "|       01-01-17|          126|                LGA|                 171|\n",
      "|       01-01-17|          132|                EWR|                 188|\n",
      "|       01-01-17|          140|                SJC|                 231|\n",
      "|       01-01-17|          174|                RDU|                 145|\n",
      "|       01-01-17|          176|                BOS|                 184|\n",
      "+---------------+-------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df = spark.read.csv('E:/STUDY/DATASETS/AA_DFW_2017_new_Departures_Short.csv')\n",
    "full_df.show()\n",
    "\n",
    "#split_df = spark.read.csv('departures_013.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv('E:/STUDY/DATASETS/AA_DFW_2017_new_Departures_Short.csv')\n",
    "df_csv.write.parquet('E:/STUDY/DATASETS/data.parquet')\n",
    "dfp = spark.read.parquet('E:/STUDY/DATASETS/data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t139359\n",
      "Time to run: 0.534921\n",
      "Total rows in split DataFrame:\t139359\n",
      "Time to run: 0.624882\n"
     ]
    }
   ],
   "source": [
    "#full_df = spark.read.csv('departures_full.txt.gz')\n",
    "#split_df = spark.read.csv('departures_013.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % df_csv.count() )\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % dfp.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: READING SPARK CONFIGURATIONS\n",
    "1: Check the name of the Spark application instance **('spark.app.name')**.\n",
    "\n",
    "2: Determine the TCP port the driver runs on **('spark.driver.port')**.\n",
    "\n",
    "3: Determine how many partitions are configured for joins.\n",
    "\n",
    "4: Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 50496\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "Using the **spark.conf** object allows you to validate the settings of a cluster without having configured it initially. This can help you know what changes should be optimized for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 500\n",
      "Partition count after change: 500\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('E:/STUDY/DATASETS/AA_DFW_2017_new_Departures_Short.csv').distinct()\n",
    "\n",
    "#after = departures_df.rdd.getNumPartitions()\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "It's important to remember that modifying the settings in Spark may change objects that already exist. Sometimes the changes only take effect after configuring a new DataFrame. Remember to test changes you make to Spark configurations to verify it does exactly what you think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: NORMAL JOINS\n",
    "1: Create a new DataFrame normal_df by joining flights_df with **airports_df**.\n",
    "\n",
    "2: Determine which type of join is used in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1397\n"
     ]
    }
   ],
   "source": [
    "flights_df = spark.read.csv('E:/STUDY/flights_small.csv', header=True) \n",
    "print(flights_df.count())\n",
    "\n",
    "airports_df = spark.read.csv('E:/STUDY/airports.csv', header=True) \n",
    "print(airports_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [dest#1249], [alt#1305], Inner, BuildRight\n",
      ":- *(2) Project [year#1238, month#1239, day#1240, dep_time#1241, dep_delay#1242, arr_time#1243, arr_delay#1244, carrier#1245, tailnum#1246, flight#1247, origin#1248, dest#1249, air_time#1250, distance#1251, hour#1252, minute#1253]\n",
      ":  +- *(2) Filter isnotnull(dest#1249)\n",
      ":     +- *(2) FileScan csv [year#1238,month#1239,day#1240,dep_time#1241,dep_delay#1242,arr_time#1243,arr_delay#1244,carrier#1245,tailnum#1246,flight#1247,origin#1248,dest#1249,air_time#1250,distance#1251,hour#1252,minute#1253] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/STUDY/flights_small.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:string,month:string,day:string,dep_time:string,dep_delay:string,arr_time:string,arr_d...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[4, string, true]))\n",
      "   +- *(1) Project [faa#1301, name#1302, lat#1303, lon#1304, alt#1305, tz#1306, dst#1307]\n",
      "      +- *(1) Filter isnotnull(alt#1305)\n",
      "         +- *(1) FileScan csv [faa#1301,name#1302,lat#1303,lon#1304,alt#1305,tz#1306,dst#1307] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/STUDY/airports.csv], PartitionFilters: [], PushedFilters: [IsNotNull(alt)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"dest\"] == airports_df[\"alt\"] )\n",
    "print(normal_df.count())\n",
    "# Show the query plan\n",
    "normal_df.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: USING BRAODCASTING ON SPARK JOINS\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's **broadcast** operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "\n",
    "1: Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "\n",
    "2: On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "\n",
    "3: If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Import the **broadcast()** method from pyspark.sql.functions.\n",
    "\n",
    "2: Create a new DataFrame **broadcast_df** by joining **flights_df** with **airports_df**, using the broadcasting.\n",
    "\n",
    "3: Show the query plan and consider differences from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [dest#985], [alt#1020], Inner, BuildRight\n",
      ":- *(2) Project [year#974, month#975, day#976, dep_time#977, dep_delay#978, arr_time#979, arr_delay#980, carrier#981, tailnum#982, flight#983, origin#984, dest#985, air_time#986, distance#987, hour#988, minute#989]\n",
      ":  +- *(2) Filter isnotnull(dest#985)\n",
      ":     +- *(2) FileScan csv [year#974,month#975,day#976,dep_time#977,dep_delay#978,arr_time#979,arr_delay#980,carrier#981,tailnum#982,flight#983,origin#984,dest#985,air_time#986,distance#987,hour#988,minute#989] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/STUDY/flights_small.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:string,month:string,day:string,dep_time:string,dep_delay:string,arr_time:string,arr_d...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[4, string, true]))\n",
      "   +- *(1) Project [faa#1016, name#1017, lat#1018, lon#1019, alt#1020, tz#1021, dst#1022]\n",
      "      +- *(1) Filter isnotnull(alt#1020)\n",
      "         +- *(1) FileScan csv [faa#1016,name#1017,lat#1018,lon#1019,alt#1020,tz#1021,dst#1022] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/E:/STUDY/airports.csv], PartitionFilters: [], PushedFilters: [IsNotNull(alt)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"dest\"] == airports_df[\"alt\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: COMPARING BROADCAST vs NORMAL JOINS\n",
    "1: Execute **.count()** on the normal DataFrame.\n",
    "\n",
    "2: Execute **.count()** on the broadcasted DataFrame.\n",
    "\n",
    "3: Print the count and duration of the DataFrames noting and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t0\tduration: 1.764707\n",
      "Broadcast count:\t0\tduration: 0.641665\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count() \n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter4: COMPLEX PROCESSING AND DATA PIPELINES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: QUICK PIPELINE\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Import the file **2015-departures.csv.gz** to a DataFrame. Note the header is already defined.\n",
    "\n",
    "2: Filter the DataFrame to contain only flights with a duration over 0 minutes. Use the index of the column, not the column name (remember to use **.printSchema()** to see the column names / order).\n",
    "\n",
    "3: Add an ID column.\n",
    "\n",
    "4: Write the file out as a JSON document named **output.json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('E:/STUDY/DATASETS/AA_DFW_2015_Departures_Short.csv', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df[3] > 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('E:/STUDY/DATASETS/output.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: REMOVING COMMENTED LINES\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Import the **annotations.csv.gz** file to a DataFrame and perform a row count. Specify a separator character of |.\n",
    "\n",
    "2: Query the data for the number of rows beginning with **#**.\n",
    "\n",
    "3: Import the file again to a new DataFrame, but specify the comment character in the options to remove any commented rows.\n",
    "\n",
    "4: Count the new DataFrame and verify the difference is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#') #Use # on the comment argument to remove comment rows.\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: REMOVING INVALID ROWS\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (**\\t**) characters.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Create a new variable **tmp_fields** using the **annotations_df** DataFrame column **'_c0'** splitting it on the tab character.\n",
    "\n",
    "2: Create a new column in **annotations_df** named **'colcount'** representing the number of fields defined in the previous step.\n",
    "\n",
    "3: Filter out any rows from **annotations_df** containing fewer than 5 fields.\n",
    "\n",
    "4: Count the number of rows in the DataFrame and compare to the **initial_count**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: SPLITTING INTO COLUMNS\n",
    "1: Split the content of the **'_c0'** column on the tab character and store in a variable called split_cols.\n",
    "\n",
    "2: Add the following columns based on the first four entries in the variable above: folder, filename, width, height on a DataFrame named **split_df**.\n",
    "\n",
    "3: Add the **split_cols** variable as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering why we're not using a schema instead to define the content layout. Spark's CSV parser can't handle advanced types (Arrays or Maps) so it wouldn't process correctly. In our example, we bypass using the types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: FURTHER PARSING\n",
    "1: Create a new function called **retriever** that takes two arguments, the split columns (cols) and the total number of columns (colcount). This function should return a list of the entries that have not been defined as columns yet (i.e., everything after item 4 in the list).\n",
    "\n",
    "2: Define the function as a Spark UDF, returning an Array of strings.\n",
    "\n",
    "3: Create the new column **dog_list** using the UDF and the available columns in the DataFrame.\n",
    "\n",
    "4: Remove the columns **_c0**, colcount, and split_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: VALIDATE ROWS VIA JOIN\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named **valid_folders_df**.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Rename the **_c0** column to **folder** on the **valid_folders_df** DataFrame.\n",
    "\n",
    "2: Count the number of rows in **split_df**.\n",
    "\n",
    "3: Join the two DataFrames on the folder name, and call the resulting DataFrame **joined_df**. Make sure to broadcast the smaller DataFrame.\n",
    "\n",
    "4: Check the number of rows remaining in the DataFrame and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: EXAMINING INVALID ROWS\n",
    "1: Determine the row counts for each DataFrame.\n",
    "\n",
    "2: Create a DataFrame containing only the invalid rows.\n",
    "\n",
    "3: Validate the count of the new DataFrame is as expected.\n",
    "\n",
    "4: Determine the number of distinct folder rows removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOG PARSING\n",
    "1: Select the column representing the dog details from the DataFrame and show the first 10 un-truncated rows.\n",
    "\n",
    "2: Create a new schema as you've done before, using breed, start_x, start_y, end_x, and end_y as the names. Make sure to specify the proper data types for each field in the schema (any number value is an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PER IMAGE COUNT\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your **dog_list** column created earlier. You have also created the **DogType** which will allow better parsing of the data within some of the data columns.\n",
    "\n",
    "The **joined_df** is available as you last defined it, and the **DogType** structtype is defined. pyspark.sql.functions is available under the F alias.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Create a Python function to split each entry in dog_list to its appropriate parts. Make sure to convert any strings into the appropriate types or the DogType will not parse correctly.\n",
    "\n",
    "2: Create a UDF using the above function.\n",
    "\n",
    "3: Use the UDF to create a new column called dogs. Drop the previous column in the same command.\n",
    "\n",
    "4: Show the number of dogs in the new column for the first 10 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "  dogs = []\n",
    "  for dog in doglist:\n",
    "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "  return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCENTAGE DOG PIXELS\n",
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "(Xend - Xstart) * (Yend - Ystart)\n",
    "\n",
    "NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
    "\n",
    "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100.\n",
    "The joined_df DataFrame is as you last used it. pyspark.sql.functions is aliased to F.\n",
    "\n",
    "### INSTRUCTIONS\n",
    "1: Define a Python function to take a list of tuples (the dog objects) and calculate the total number of \"dog\" pixels per image.\n",
    "\n",
    "2: Create a UDF of the function and use it to create a new column called **'dog_pixels'** on the DataFrame.\n",
    "\n",
    "3: Create another column, **'dog_percent'**, representing the percentage of **'dog_pixels'** in the image. Make sure this is between 0-100%.\n",
    "\n",
    "4: Show the first 10 rows with more than 60% **'dog_pixels'** in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "  totalpixels = 0\n",
    "  for dog in doglist:\n",
    "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "  return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
